{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vmnu2zn206Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import Conv2d, MaxPool2d, Linear, ReLU, Softmax, Module, BatchNorm2d, Dropout, LeakyReLU, Sequential\n",
        "from torch.nn.init import kaiming_uniform_, constant_, xavier_uniform_\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "import sys\n",
        "from torch import save, load, cuda\n",
        "from torch import device\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import torch.optim \n",
        "import torchvision.models as models\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train\"\n",
        "path_valid = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test\"\n",
        "path_train_vid = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset_Vid_50/train\"\n",
        "path_valid_vid = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset_Vid_50/test\"\n",
        "\n",
        "clip_n_frames = 50\n",
        "clip_time = 5\n",
        "count_videos = 0\n"
      ],
      "metadata": {
        "id": "KMuvGo4F_9o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcpG_aBBYkux"
      },
      "outputs": [],
      "source": [
        "valid_transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnhRvRea4mB4",
        "outputId": "51bec199-3c48-40da-a008-06a6a37f3910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Boxing\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Eating\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Clapping\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Waving\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Hugging\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Jumping\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Laughing\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Smoking\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Walking\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "count_video = 0\n",
        "for path_category in glob.glob(path_train + '/*'):\n",
        "    print(path_category)\n",
        "    category = path_category.split(\"/\")[-1]\n",
        "    for path in glob.glob(path_category + '/*'):\n",
        "        # print(path)\n",
        "        vidcap = cv2.VideoCapture(path)\n",
        "          \n",
        "        # count the number of frames\n",
        "        frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        # print(f'fps:{fps}, frames:{frames}')\n",
        "\n",
        "        count_videos += 1\n",
        "        \n",
        "        vidcap = cv2.VideoCapture(path)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        success, image = vidcap.read()\n",
        "        frames = []\n",
        "        \n",
        "        time = []\n",
        "        count = 0  # control to have the same number of frames\n",
        "        count_fps = 0\n",
        "        count_sections = 0\n",
        "        while success:\n",
        "            \n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "            if(type(image).__module__ == np.__name__):\n",
        "              new_image = valid_transform(Image.fromarray(image))\n",
        "              frames.append(new_image)\n",
        "              count_fps += 1\n",
        "\n",
        "        num_frames = len(frames)\n",
        "        video_time = num_frames/fps\n",
        "        \n",
        "        if(num_frames < 15):\n",
        "          print(num_frames)\n",
        "          continue\n",
        "\n",
        "        if num_frames < clip_n_frames:\n",
        "          while (count > 0 and count <= clip_n_frames):\n",
        "            frames.append(new_image)    # if the number of frames is lower than the num_frames, repeat the last image to reach num_frames\n",
        "            count +=1\n",
        "            count_fps += 1\n",
        "            \n",
        "          video_file = torch.zeros((3, 224, 224*clip_n_frames))\n",
        "          for i in range(clip_n_frames):\n",
        "            video_file[:,:,i*224:(i+1)*224] = frames[i]\n",
        "\n",
        "          name_video = category + str(count_video) + \".png\"\n",
        "          path_directory = os.path.join(path_train_vid, category)\n",
        "          if not os.path.exists(path_directory):\n",
        "              os.makedirs(path_directory)\n",
        "          path_video = os.path.join(path_directory, name_video)\n",
        "          save_image(video_file, path_video)\n",
        "\n",
        "          # print(f'count_video:{count_video}, frames:{len(frames)}, images:{frames[0].shape}')\n",
        "          count_video += 1\n",
        "\n",
        "        else:\n",
        "          num_sections = math.ceil(video_time/clip_time)\n",
        "          if num_sections == 0:\n",
        "            num_sections = 1\n",
        "          frame_rate = int(num_frames/(clip_n_frames*num_sections))\n",
        "          while (frame_rate == 0):\n",
        "            num_sections -= 1\n",
        "            frame_rate = int(num_frames/(clip_n_frames*num_sections))\n",
        "\n",
        "          while (count_sections < num_sections):\n",
        "            videos_3d = frames[count_sections*clip_n_frames*frame_rate:(count_sections+1)*clip_n_frames*frame_rate:frame_rate]\n",
        "            count_sections += 1\n",
        "              \n",
        "            video_file = torch.zeros((3, 224, 224*clip_n_frames))\n",
        "            for i in range(clip_n_frames):\n",
        "              video_file[:,:,i*224:(i+1)*224] = videos_3d[i]\n",
        "\n",
        "            name_video = category + str(count_video) + \".png\"\n",
        "            path_directory = os.path.join(path_train_vid, category)\n",
        "            if not os.path.exists(path_directory):\n",
        "                os.makedirs(path_directory)\n",
        "            path_video = os.path.join(path_directory, name_video)\n",
        "            save_image(video_file, path_video)\n",
        "\n",
        "            count_video += 1\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmXBXtt85JJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b925b2-60bb-4265-b4b9-8469fb06ba43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Boxing\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Eating\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Clapping\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Waving\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Hugging\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Jumping\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Laughing\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Smoking\n",
            "/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Walking\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "count_video = 0\n",
        "for path_category in glob.glob(path_valid + '/*'):\n",
        "    print(path_category)\n",
        "    category = path_category.split(\"/\")[-1]\n",
        "    for path in glob.glob(path_category + '/*'):\n",
        "        # print(path)\n",
        "        vidcap = cv2.VideoCapture(path)\n",
        "          \n",
        "        # count the number of frames\n",
        "        frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        # print(f'fps:{fps}, frames:{frames}')\n",
        "\n",
        "        count_videos += 1\n",
        "        \n",
        "        vidcap = cv2.VideoCapture(path)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        success, image = vidcap.read()\n",
        "        frames = []\n",
        "        \n",
        "        time = []\n",
        "        count = 0  # control to have the same number of frames\n",
        "        count_fps = 0\n",
        "        count_sections = 0\n",
        "        while success:\n",
        "            \n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "            if(type(image).__module__ == np.__name__):\n",
        "              new_image = valid_transform(Image.fromarray(image))\n",
        "              frames.append(new_image)\n",
        "              count_fps += 1\n",
        "\n",
        "        num_frames = len(frames)\n",
        "        video_time = num_frames/fps\n",
        "        \n",
        "        if(num_frames < 20):\n",
        "          print(num_frames)\n",
        "          continue\n",
        "\n",
        "        if num_frames < clip_n_frames:\n",
        "          while (count > 0 and count <= clip_n_frames):\n",
        "            frames.append(new_image)    # if the number of frames is lower than the num_frames, repeat the last image to reach num_frames\n",
        "            count +=1\n",
        "            count_fps += 1\n",
        "            \n",
        "          video_file = torch.zeros((3, 224, 224*clip_n_frames))\n",
        "          for i in range(clip_n_frames):\n",
        "            video_file[:,:,i*224:(i+1)*224] = frames[i]\n",
        "\n",
        "          name_video = category + str(count_video) + \".png\"\n",
        "          path_directory = os.path.join(path_valid_vid, category)\n",
        "          if not os.path.exists(path_directory):\n",
        "              os.makedirs(path_directory)\n",
        "          path_video = os.path.join(path_directory, name_video)\n",
        "          save_image(video_file, path_video)\n",
        "\n",
        "          # print(f'count_video:{count_video}, frames:{len(frames)}, images:{frames[0].shape}')\n",
        "          count_video += 1\n",
        "\n",
        "        else:\n",
        "          num_sections = math.ceil(video_time/clip_time)\n",
        "          if num_sections == 0:\n",
        "            num_sections = 1\n",
        "          frame_rate = int(num_frames/(clip_n_frames*num_sections))\n",
        "          while (frame_rate == 0):\n",
        "            num_sections -= 1\n",
        "            frame_rate = int(num_frames/(clip_n_frames*num_sections))\n",
        "\n",
        "          while (count_sections < num_sections):\n",
        "            videos_3d = frames[count_sections*clip_n_frames*frame_rate:(count_sections+1)*clip_n_frames*frame_rate:frame_rate]\n",
        "            count_sections += 1\n",
        "              \n",
        "            video_file = torch.zeros((3, 224, 224*clip_n_frames))\n",
        "            for i in range(clip_n_frames):\n",
        "              video_file[:,:,i*224:(i+1)*224] = videos_3d[i]\n",
        "\n",
        "            name_video = category + str(count_video) + \".png\"\n",
        "            path_directory = os.path.join(path_valid_vid, category)\n",
        "            if not os.path.exists(path_directory):\n",
        "                os.makedirs(path_directory)\n",
        "            path_video = os.path.join(path_directory, name_video)\n",
        "            save_image(video_file, path_video)\n",
        "\n",
        "            count_video += 1\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separating one folder of image or video to train and test\n",
        "# from os import walk\n",
        "\n",
        "# # import OS module\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# # Get the list of all files and directories\n",
        "# mypath = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/train/walk/\"\n",
        "# train_path = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/train/Walking/\"\n",
        "# test_path = \"/content/drive/MyDrive/Elaheh/Deep_Learning_Project/Dataset/test/Walking/\"\n",
        "# dir_list = os.listdir(mypath)\n",
        "\n",
        "# print(\"Files and directories in '\", mypath, \"' :\")\n",
        "\n",
        "# # prints all files\n",
        "# FileNames = []\n",
        "# for file in dir_list:\n",
        "#     FileNames.append(file)\n",
        "#     # print(file)\n",
        "\n",
        "# len_dataset = len(FileNames)\n",
        "# len_train = int(len_dataset * 0.9)\n",
        "# len_test = len_dataset-len_train\n",
        "\n",
        "# if not os.path.exists(train_path):\n",
        "#     os.mkdir(train_path)\n",
        "# if not os.path.exists(test_path):\n",
        "#     os.mkdir(test_path)\n",
        "\n",
        "# for i in range(len_dataset):\n",
        "#     if i < len_train:\n",
        "#         path = os.path.join(mypath + FileNames[i])\n",
        "#         shutil.copy(path, train_path)\n",
        "#     else:\n",
        "#         path = os.path.join(mypath + FileNames[i])\n",
        "#         shutil.copy(path, test_path)"
      ],
      "metadata": {
        "id": "h_Yn7gsySxDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1394yMnCllii"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Elaheh_DL_Proj_Preprocessing_9_Categories.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}